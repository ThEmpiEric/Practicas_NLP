{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 5\n",
    "\n",
    "Eric Lemus Avalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias necesarias \n",
    "import os \n",
    "import time \n",
    "import shutil \n",
    "import random \n",
    "from typing import Tuple \n",
    "from argparse import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "\n",
    "from nltk import ngrams # extraer los N-gramas\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repricabilidad \n",
    "seed = 777\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Este ultimo atributo de PyTorch permite activar o desactivar una optimizaciÃ³n especÃ­fica para seleccionar \n",
    "# las mejores configuraciones de algoritmos de cuDNN en funciÃ³n del tamaÃ±o de los tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)          \n",
    "print(torch.version.cuda) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajamos sobre los datos (tweets) de la practica 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = r'C:\\Users\\ericl\\anaconda3\\envs\\nlp\\practicas\\Practica_4\\Data\\mex_train.txt'\n",
    "path_val = r'C:\\Users\\ericl\\anaconda3\\envs\\nlp\\practicas\\Practica_4\\Data\\mex_val.txt'\n",
    "path_trg_val = r'C:\\Users\\ericl\\anaconda3\\envs\\nlp\\practicas\\Practica_4\\Data\\mex_val_labels.txt'\n",
    "path_trg_train = r'C:\\Users\\ericl\\anaconda3\\envs\\nlp\\practicas\\Practica_4\\Data\\mex_train_labels.txt'\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(path_train, header=None)\n",
    "df_trg_train = pd.read_csv(path_trg_train, header=None)\n",
    "df_val = pd.read_csv(path_val, header=None)\n",
    "df_trg_val = pd.read_csv(path_trg_val, header=None)\n",
    "\n",
    "x_train = df_train[0].tolist()\n",
    "x_val   = df_val[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl',\n",
       " 'a la vga no seas mamÃ³n 45 putos minutos despuÃ©s me dices que apenas sales no me querÃ­as avisar en 3 horas? ðŸ˜‘',\n",
       " 'considero que lo mÃ¡s conveniente seria que lo retes a unos vergazos mi jelipe! rÃ³mpele la madre a ese pinchi joto!',\n",
       " 'el marica de mi ex me tiene bloqueada de todo asÃ­  uno no puede admirar la \"belleza\" de su garnacha ðŸ˜‚',\n",
       " 'mujer despechadaya pinche amlo hazle esta que se pela la loca #reynosafollow #reynosa',\n",
       " 'putos. no tienen madre. ambriados mantenidos. ojetes. como es posible. mejor matarlos',\n",
       " 'ustedes si puden andar de chanceros pero cuidadito y seamos nosotras porque luego luego empiezan a mamar hijos de la chingada.',\n",
       " '@usuario jajjaja te digo esa madre si estÃ¡ buena ajjaja',\n",
       " 'odio los putos trÃ¡mites de titulaciÃ³n ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡ pero me urge la precedula.',\n",
       " '@usuario no te equivocabas mi madre y tu tenÃ­an muchÃ­sima razÃ³n siempre es mejor lo que viene ðŸ’š']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observamos una muestra de los datos (tweets)\n",
    "x_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tetagramas: creamos objeto Namespace vacio y creamos el arg N. \n",
    "args = Namespace()\n",
    "args.N = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El archivo NgramData.py contiene la clase del mismo nombre utilizada para crear los n-gramas\n",
    "from NgramData import NgramData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto TweetTokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = TweetTokenizer(reduce_len = True)\n",
    "tokenizador = tok.tokenize\n",
    "ngram_data = NgramData(args.N, 5000, tokenizador)\n",
    "ngram_data.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocal size:  5000\n"
     ]
    }
   ],
   "source": [
    "print('Vocal size: ', ngram_data.get_vocab_size())\n",
    "# ngram_data.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ta',\n",
       " 'alfa',\n",
       " 'puedes',\n",
       " 'caricias',\n",
       " 'regalar',\n",
       " 'wau',\n",
       " 'libro',\n",
       " 'conoces',\n",
       " 'mande',\n",
       " 'Ã©tica']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngram_data.vocab)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lo': 0, 'peor': 1, 'de': 2, 'todo': 3, 'es': 4, 'que': 5, 'no': 6, 'me': 7, 'dan': 8, 'por': 9}\n"
     ]
    }
   ],
   "source": [
    "w = ngram_data.word2id \n",
    "muestra_m = dict(list(w.items())[:10])\n",
    "print(muestra_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** Necesitamos dejar los tokens listos para ser tomado por el modelo. Funcion transform de la clase (token -> seq).\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "Ejemplo: \n",
    "\n",
    " ```python\n",
    "                                  [\n",
    "                                    ['<s>', '<s>', '<s>'],\n",
    "                                    ['<s>', '<s>', 'lo'],\n",
    " Lo peor de todo es que no se ->    ['<s>', 'lo', 'peor'],\n",
    "                                    ['lo', 'peor', 'de'],\n",
    "                                    ['peor', 'de', 'todo'],\n",
    "                                    ['de', 'todo', 'es'],\n",
    "                                    ['todo', 'es', 'que'],\n",
    "                                    ['es', 'que', 'no'],\n",
    "                                    ['que', 'no', 'me']\n",
    "                                                         ]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset y Dataloader \n",
    "\n",
    "\n",
    "Preparamos los datos para ingresarlos a un TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(x_train)\n",
    "X_ngram_val, y_ngram_val   = ngram_data.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_ngram_train: (106823, 3)\n",
      "Shape of y_ngram_train: (106823,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_ngram_train: {X_ngram_train.shape}')\n",
    "print(f'Shape of y_ngram_train: {y_ngram_train.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_ngram_val: (11587, 3)\n",
      "Shape of y_ngram_val: (11587,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_ngram_val: {X_ngram_val.shape}')\n",
    "print(f'Shape of y_ngram_val: {y_ngram_val.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', '<s>', '<s>'],\n",
       " ['<s>', '<s>', 'lo'],\n",
       " ['<s>', 'lo', 'peor'],\n",
       " ['lo', 'peor', 'de'],\n",
       " ['peor', 'de', 'todo'],\n",
       " ['de', 'todo', 'es'],\n",
       " ['todo', 'es', 'que'],\n",
       " ['es', 'que', 'no'],\n",
       " ['que', 'no', 'me'],\n",
       " ['no', 'me', 'dan'],\n",
       " ['me', 'dan', 'por'],\n",
       " ['dan', 'por', 'un'],\n",
       " ['por', 'un', 'tiempo'],\n",
       " ['un', 'tiempo', 'y'],\n",
       " ['tiempo', 'y', 'luego'],\n",
       " ['y', 'luego', 'vuelven'],\n",
       " ['luego', 'vuelven', 'estoy'],\n",
       " ['vuelven', 'estoy', 'hasta'],\n",
       " ['estoy', 'hasta', 'la'],\n",
       " ['hasta', 'la', 'verga']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Podemos aplicar un mapeo para ver las palabras \n",
    "[[ngram_data.id2word[id] for id in lista] for lista in X_ngram_train[:20]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 32\n",
    "args.num_workers = 2\n",
    "\n",
    "# Dataset \n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype= torch.int64), \n",
    "                              torch.tensor(y_ngram_train, dtype= torch.int64))\n",
    "\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype= torch.int64), \n",
    "                            torch.tensor(y_ngram_val, dtype= torch.int64))\n",
    "\n",
    "# Loader \n",
    "train_loader =  DataLoader(train_dataset, \n",
    "                           batch_size = args.batch_size, \n",
    "                           num_workers = args.num_workers, \n",
    "                           shuffle = True, pin_memory=False)\n",
    "\n",
    "valid_loader =  DataLoader(val_dataset, \n",
    "                         batch_size = args.batch_size, \n",
    "                         num_workers = args.num_workers, \n",
    "                         shuffle = False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 3])\n",
      "y shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f'X shape: {batch[0].shape}')\n",
    "print(f'y shape: {batch[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['de', 'calor', 'encima'],\n",
       " ['bastardo', 'de', '<unk>'],\n",
       " ['con', 'sus', 'quejas'],\n",
       " ['quiero', 'vivir', 'sola'],\n",
       " ['la', 'que', 'no'],\n",
       " ['mil', 'putas', 'juntas'],\n",
       " ['empedar', 'la', 'vida'],\n",
       " ['me', 'gusta', 'alguien'],\n",
       " ['<s>', '<s>', 'hola'],\n",
       " ['la', 'madre', 'no'],\n",
       " ['a', '<unk>', '<unk>'],\n",
       " ['a', 'ti', 'te'],\n",
       " ['de', 'la', 'verga'],\n",
       " ['<s>', 'putos', 'chairos'],\n",
       " ['<s>', 'este', 'hdp'],\n",
       " ['de', '<unk>', 'donde'],\n",
       " ['de', 'los', 'videos'],\n",
       " ['poner', 'al', 'tu'],\n",
       " ['weyes', 'a', 'lado'],\n",
       " ['se', 'vienen', 'los']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muestra_batch = batch[0]\n",
    "# Podemos usar el diccionario id2word para ver el contentenido.\n",
    "[[ngram_data.id2word[id] for id in lista] for lista in muestra_batch.tolist()][:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5027,  0.7398,  0.0321],\n",
      "        [-0.7081,  0.9164, -2.3439],\n",
      "        [ 0.4917, -1.1725, -0.7897],\n",
      "        [ 0.7475, -1.8150, -0.8740],\n",
      "        [ 2.1855,  0.2381, -0.4865],\n",
      "        [-0.3414, -0.8560,  0.8810],\n",
      "        [-1.1504,  0.8726, -0.2438],\n",
      "        [ 0.7136,  0.3745,  1.3340],\n",
      "        [ 0.6958,  1.3113,  0.0672],\n",
      "        [-1.0748, -1.1580, -0.3790]], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Hacemos lo equivalente a la operacion de usar vectores one-hot\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5027,  0.7398,  0.0321],\n",
       "        [ 0.4917, -1.1725, -0.7897],\n",
       "        [-0.3414, -0.8560,  0.8810]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de como se ve los embeddings\n",
    "e = nn.Embedding(10,3)\n",
    "print(e.weight) \n",
    "\n",
    "print('-----'*10)\n",
    "print('\\n')\n",
    "\n",
    "print('Hacemos lo equivalente a la operacion de usar vectores one-hot\\n')\n",
    "e(torch.tensor([0,2,5])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8000, 3.2000, 4.4000, 4.3000, 3.3000], requires_grad=True)\n",
      "tensor([7.8000, 3.2000, 4.4000, 4.3000, 3.3000])\n"
     ]
    }
   ],
   "source": [
    "# detach(): Returns a new Tensor, detached from the current graph.\n",
    "tensor1 = torch.tensor([7.8, 3.2, 4.4, 4.3, 3.3], requires_grad=True) \n",
    "print(tensor1) \n",
    "  \n",
    "# detach the tensor \n",
    "print(tensor1.detach()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Bengio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_NLM_Bengio(nn.Module): \n",
    "    def __init__(self, \n",
    "                 args):\n",
    "        super(Model_NLM_Bengio, self).__init__()\n",
    "        self.window_size = args.N - 1\n",
    "        self.embedding_size = args.d_embeddings\n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.d_embeddings)\n",
    "        self.fc1 = nn.Linear(args.d_embeddings*(args.N-1),args.d_h)\n",
    "        self.drop1 = nn.Dropout(p = args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.embedding(x) #embedings en forma de matriz\n",
    "        x = x.view(-1, self.window_size*self.embedding_size) # Ponerlo en un vector del tamano necesari \n",
    "        h = F.relu(self.fc1(x)) # modelo de bengio, ReLU(x) = max(0,x)\n",
    "        h = self.drop1(h)\n",
    "        return self.fc2(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener una distibucion de probabilidade de los logits \n",
    "def get_preds(raw_logits): \n",
    "    '''\n",
    "    Funcion para obetener la distribucion de probabilidad de los logits del modelo \n",
    "    '''\n",
    "    probs = F.softmax(raw_logits.detach(), dim = 1)\n",
    "    y_pred = torch.argmax(probs, dim = 1).cpu().numpy()\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluacion del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(data,model, gpu = False):\n",
    "    with torch.no_grad(): \n",
    "        preds, trgs = [], [] \n",
    "        for window_words, labels in data: \n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "\n",
    "            outputs = model(window_words)\n",
    "\n",
    "            # Get predictions \n",
    "            y_pre = get_preds(outputs)\n",
    "            preds.append(y_pre)\n",
    "\n",
    "            trg  = labels.numpy()\n",
    "            trgs.append(trg)\n",
    "    # Aplanar el batch \n",
    "    tgts = [e for l in trgs for e in l]\n",
    "    preds = [e for l in preds for e in l]\n",
    "            \n",
    "    return accuracy_score(tgts,preds)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_patn, filename = 'checkpoint.pt'):\n",
    "    '''Funcion para serializar objetos a disco y guardar ese objeto'''\n",
    "    filename = os.path.join(checkpoint_patn, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best: \n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_patn, 'model_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "args.d_embeddings = 100\n",
    "args.d_h = 200\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Train hyperparameters \n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "# Scheduler hyperparameters \n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5 \n",
    "\n",
    "# save directory\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Model\n",
    "model = Model_NLM_Bengio(args)\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "     model.cuda()\n",
    "else:\n",
    "     model.cpu() \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizador = torch.optim.SGD(model.parameters(), lr = args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizador, mode = 'min', \n",
    "                                                      patience = args.lr_patience, \n",
    "                                                      verbose = True,  \n",
    "                                                      factor = args.lr_factor) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.14828675180764128\n",
      "Epoch [1/100], Loss: 5.5514 - Val accuracy: 0.1801 - Epoch time: 13.97\n",
      "Train acc: 0.16396322680015402\n",
      "Epoch [2/100], Loss: 5.1085 - Val accuracy: 0.1886 - Epoch time: 14.07\n",
      "Train acc: 0.170683053095452\n",
      "Epoch [3/100], Loss: 4.8670 - Val accuracy: 0.1952 - Epoch time: 13.94\n",
      "Train acc: 0.17637337954049542\n",
      "Epoch [4/100], Loss: 4.6739 - Val accuracy: 0.1768 - Epoch time: 14.36\n",
      "Train acc: 0.17969585632995336\n",
      "Epoch [5/100], Loss: 4.5045 - Val accuracy: 0.1887 - Epoch time: 14.18\n",
      "Train acc: 0.18510006203739357\n",
      "Epoch [6/100], Loss: 4.3469 - Val accuracy: 0.1988 - Epoch time: 14.07\n",
      "Train acc: 0.18974618149146452\n",
      "Epoch [7/100], Loss: 4.2001 - Val accuracy: 0.1446 - Epoch time: 14.35\n",
      "Train acc: 0.19519049758268087\n",
      "Epoch [8/100], Loss: 4.0738 - Val accuracy: 0.2114 - Epoch time: 14.01\n",
      "Train acc: 0.2049694091473067\n",
      "Epoch [9/100], Loss: 3.9507 - Val accuracy: 0.1296 - Epoch time: 14.00\n",
      "Train acc: 0.21405708509819021\n",
      "Epoch [10/100], Loss: 3.8444 - Val accuracy: 0.1832 - Epoch time: 14.16\n",
      "Train acc: 0.22517167244256192\n",
      "Epoch [11/100], Loss: 3.7447 - Val accuracy: 0.1556 - Epoch time: 14.22\n",
      "Train acc: 0.23441845291575747\n",
      "Epoch [12/100], Loss: 3.6630 - Val accuracy: 0.1480 - Epoch time: 14.16\n",
      "Train acc: 0.24619619860522823\n",
      "Epoch [13/100], Loss: 3.5819 - Val accuracy: 0.1295 - Epoch time: 14.11\n",
      "Train acc: 0.25478784281008\n",
      "Epoch [14/100], Loss: 3.5213 - Val accuracy: 0.1790 - Epoch time: 14.06\n",
      "Train acc: 0.2629449578573568\n",
      "Epoch [15/100], Loss: 3.4518 - Val accuracy: 0.1553 - Epoch time: 14.08\n",
      "Train acc: 0.2707089911436273\n",
      "Epoch [16/100], Loss: 3.3955 - Val accuracy: 0.1992 - Epoch time: 14.23\n",
      "Train acc: 0.2766974714414067\n",
      "Epoch [17/100], Loss: 3.3457 - Val accuracy: 0.1883 - Epoch time: 14.35\n",
      "Train acc: 0.2833183587900569\n",
      "Epoch [18/100], Loss: 3.2992 - Val accuracy: 0.1476 - Epoch time: 15.86\n",
      "Train acc: 0.2919487763658922\n",
      "Epoch [19/100], Loss: 3.2528 - Val accuracy: 0.1556 - Epoch time: 15.86\n",
      "Train acc: 0.2976297437213879\n",
      "Epoch [20/100], Loss: 3.2121 - Val accuracy: 0.1673 - Epoch time: 16.14\n",
      "Train acc: 0.3014442412185\n",
      "Epoch [21/100], Loss: 3.1756 - Val accuracy: 0.1340 - Epoch time: 15.95\n",
      "Train acc: 0.30856116031318187\n",
      "Epoch [22/100], Loss: 3.1372 - Val accuracy: 0.1841 - Epoch time: 14.70\n",
      "Train acc: 0.3120387305865743\n",
      "Epoch [23/100], Loss: 3.1075 - Val accuracy: 0.1904 - Epoch time: 17.34\n",
      "Train acc: 0.32027339237581826\n",
      "Epoch [24/100], Loss: 3.0667 - Val accuracy: 0.1509 - Epoch time: 17.79\n",
      "Train acc: 0.3753770376074958\n",
      "Epoch [25/100], Loss: 2.7050 - Val accuracy: 0.1723 - Epoch time: 17.95\n",
      "Train acc: 0.38538056732126813\n",
      "Epoch [26/100], Loss: 2.6271 - Val accuracy: 0.1770 - Epoch time: 18.96\n",
      "Train acc: 0.38977933940871945\n",
      "Epoch [27/100], Loss: 2.5988 - Val accuracy: 0.1791 - Epoch time: 18.16\n",
      "No improvement. Breaking out of loop.\n",
      "--- 427.53274273872375 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "\n",
    "    for window_words, labels in train_loader:\n",
    "\n",
    "        # If GPU available\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "        # Get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizador.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizador.step()\n",
    "\n",
    "    # Get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    # Get metric in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(valid_loader, model, gpu = args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    # Update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    # Check for metric improvement\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "\n",
    "    # Save best model if metric improved\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizador.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of loop.\")\n",
    "        break\n",
    "\n",
    "    print('Train acc: {}'.format(mean_epoch_metric))\n",
    "    print('Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}'\n",
    "          .format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time() - epoch_start_time)))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploracion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_word(embedding,ngram_data, word, n):\n",
    "     wors_id = torch.LongTensor([ngram_data.word2id[word]])\n",
    "     word_embed = embedding(wors_id)\n",
    "     distancia = torch.norm(embedding.weight - word_embed, dim = 1).detach()\n",
    "     words_list = sorted(enumerate(distancia.numpy()), key = lambda x:x[1])\n",
    "     for idx, diff in words_list[1:n+1]:\n",
    "          print(ngram_data.id2word[idx], diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericl\\AppData\\Local\\Temp\\ipykernel_29192\\466064971.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load(bets_model_path)['state_dict'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model_NLM_Bengio(\n",
       "  (embedding): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos el mejor modelo con los embeddings ya entrenados \n",
    "best_model = Model_NLM_Bengio(args)\n",
    "bets_model_path = r'C:\\Users\\ericl\\anaconda3\\envs\\nlp\\practicas\\Practica_5\\model\\model_best.pt'\n",
    "best_model.load_state_dict(torch.load(bets_model_path)['state_dict'])\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Learned Embeddings\n",
      "--------------------\n",
      "<s> 10.153593\n",
      "<unk> 10.441092\n",
      "que 10.998937\n",
      "de 11.083012\n",
      "parece 11.109263\n",
      "aguas 11.172365\n",
      "jajajaa 11.212874\n",
      "ptm 11.261413\n",
      "tendrÃ© 11.264101\n",
      "cama 11.266591\n"
     ]
    }
   ],
   "source": [
    "print('-'*20)\n",
    "print('Learned Embeddings')\n",
    "print('-'*20)\n",
    "print_closest_word(best_model.embedding, ngram_data, 'perro', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizarlo para generar texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text,tokenizador): \n",
    "    '''Funcion para sacar los tokens del texto'''\n",
    "    all_tokens = [w.lower() if w in ngram_data.word2id else '<unk>' for w in tokenizador.tokenize(text)]\n",
    "    tokens_ids = [ngram_data.word2id[token.lower()] for token in all_tokens]\n",
    "    return all_tokens, tokens_ids\n",
    "\n",
    "\n",
    "def sample_next_word(logits, temperature = 1.0): \n",
    "    logits = np.asarray(logits).astype('float64')\n",
    "    preds = logits / temperature #temperature es un factor de estocasidad de los tokens\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def predic_next_token(model, token_ids):\n",
    "    word_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "\n",
    "    #y_probs = F.softmax(y_raw_pred, dim = 1)\n",
    "    #y_pred = torch.argmax(y_probs, dim = 1).detach().numpy()\n",
    "\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.0)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def generate_text(model, initial_text, tokenizador, n_pred = 100): \n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizador)\n",
    "\n",
    "    for i in range(n_pred): \n",
    "        y_pred = predic_next_token(model,window_word_ids)\n",
    "        next_word = ngram_data.id2word[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "\n",
    "        if next_word == '</s>':\n",
    "            break \n",
    "        else: \n",
    "            window_word_ids.pop(0) #Eliminar el ultimo elemento\n",
    "            window_word_ids.append(y_pred) \n",
    "    \n",
    "    return ' '.join(all_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Learned Embeddings\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> <s> <s> porq todos <unk> ðŸ˜¡ <unk> valiÃ³ verga en <unk> hijo de tu puta pinche </s>'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = TweetTokenizer()\n",
    "initial_text = '<s> <s> <s>'\n",
    "print('-'*20)\n",
    "print('Learned Embeddings')\n",
    "print('-'*20)\n",
    "generate_text(best_model,initial_text,tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Learned Embeddings\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> <s> hijo de verga </s>'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_text = '<s> <s> hijo'\n",
    "print('-'*20)\n",
    "print('Learned Embeddings')\n",
    "print('-'*20)\n",
    "generate_text(best_model,initial_text,tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Learned Embeddings\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> hola <unk> <unk> <unk> se borrar a tu maricon <unk> chinga y abel ahi sale <unk> <unk> que tÃº darte <unk> puta arriba y yo putas que se piojo todo el amigo de algunas <unk> <unk> <unk> ratas y estÃ¡s sin basta <unk> en excelentes ... se <unk> esas puta <unk> <unk> </s>'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_text = '<s> hola python'\n",
    "print('-'*20)\n",
    "print('Learned Embeddings')\n",
    "print('-'*20)\n",
    "generate_text(best_model,initial_text,tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos tener una especie de probabilidad que nos diga que tan probable es tener una seq de palabra dado \n",
    "\n",
    "un contexto (nuestro modelo previamente entrenado con los tweets). \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Nota:** La log verosimilitud no es una probabilidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_data):\n",
    "    # Genera una ventana de n-gram \n",
    "    X, y = ngram_data.transform([text])\n",
    "    # Desechamos los dos primeros n-gramas ya que contienen el token '<s>'\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "\n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -18.209625\n"
     ]
    }
   ],
   "source": [
    "llh = log_likelihood(best_model,\"Arriva las chivas\", ngram_data)\n",
    "print(\"log likelihood: \", llh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -13.571409\n"
     ]
    }
   ],
   "source": [
    "llh = log_likelihood(best_model,\"csm el america\", ngram_data)\n",
    "print(\"log likelihood: \", llh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructuras sintacticas correctas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "-41.57523 pierde gana si el chivas no mi familia\n",
      "-42.840347 familia gana si no el pierde chivas mi\n",
      "-43.065834 familia pierde si no el chivas mi gana\n",
      "-43.298546 mi pierde gana si el chivas no familia\n",
      "-43.375275 familia chivas no mi pierde gana si el\n",
      "--------------------\n",
      "-86.67063 si chivas gana familia pierde mi el no\n",
      "-86.78021 no chivas familia gana pierde mi el si\n",
      "-87.30734 si no chivas gana familia pierde mi el\n",
      "-87.855225 si no chivas familia gana pierde el mi\n",
      "-87.99476 no chivas gana familia pierde mi el si\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "word_seq = \"si no gana el chivas pierde mi familia\".split(' ')\n",
    "perms = [' '.join(perm) for perm in permutations(word_seq)]\n",
    "\n",
    "print('-'*20)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]: \n",
    "    print(p, t)\n",
    "\n",
    "\n",
    "# Tomamaos los ultimos \n",
    "print('-'*20)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]: \n",
    "    print(p, t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
